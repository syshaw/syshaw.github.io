---
title: 十大经典算法之一：逻辑回归
categories: [机器学习]
comments: true
---
## 定义
Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。
因其简单、可并行化、可解释强深受工业界喜爱。
统计的方法，线性分类

关键字：LR不依赖样本之间的距离

### 极大似然估计法：
通过观察到的数据来选择模型最优参数的方法。
其基本思想是通过找到最大化给定数据概率的模型参数来选择最合适的模型。
具体来说，给定一个数据集和一个参数化的概率分布模型，极大似然估计的目标是找到能够最大化给定数据下该模型产生的概率的参数值。

对数似然函数：
在实际应用中，极大似然估计方法通常通过对数似然函数来实现，因为对数似然函数具有更好的数学性质和更方便的计算。
通过求对数似然函数的梯度可以得到最大似然估计的参数值。

chatGPT给的一个例子：
举一个简单的例子来说明极大似然估计法：
假设我们有一个骰子，我们并不知道骰子每个面向上的概率是多少，我们想要用一些实验数据来估计骰子每个面向上的概率。
我们可以进行若干次掷骰子的实验，记录每次掷骰子的结果。

假设我们通过实验数据计算出骰子每个面朝上的频次，那么对于均匀骰子，每个面向上的概率相等，我们可以将每个面向上的概率取为1/6，

但是如果我们在实验中观测到的数据不同，那么我们将得到不同的频次分布和概率分布，
这时我们需要通过对似然函数进行最大化来确定最优的参数估计值，
即哪个参数值能够最好地描述观测数据的概率分布。

我们定义每个面向上的概率为参数θ，假设骰子是均匀的（即每个面向上的概率相等），那么我们可以用如下的方法来进行极大似然估计：
通过实验数据计算出每个面向上的次数（即每个面出现的频次），然后将这些次数除以实验总次数，得到每个面向上的相对频率，这些相对频率即为每个面向上的概率的估计值。
比如我们掷了100次骰子，其中有20次是1点朝上，30次是2点朝上，50次是3点朝上，那么我们可以估计骰子每个面向上的概率为1/5, 3/10, 1/2。

我们想要估计的是每个面向上的概率，也就是骰子每个面出现的概率。在极大似然估计方法中，我们通过最大化观测数据给定模型的条件概率（似然函数）来选择模型参数的值，从而得到最优的参数估计值。
因此，在这个例子中，我们想要通过观测每个面向上出现的次数（观测数据）来选择最优的骰子每个面向上的概率（模型参数）。

### 损失函数
损失函数是机器学习中用来衡量预测结果（预测值）和真实结果（真实值）之间差异的一种函数。
它通常定义为模型对于每个训练样本的预测值与真实值之间的差异度量。
因此，损失函数的目的是帮助我们评估模型的预测性能，以便在训练模型时进行优化。
在极大似然估计中，我们通常使用负对数似然作为损失函数，它是似然函数的相反数，即：

L(θ) = -log L(θ|X)

其中，θ是模型的参数，X是观测数据集，L是似然函数。

这个损失函数可以用来度量在给定观测数据下参数θ产生估计值的好坏。损失函数越小，似然函数就越大，即模型拟合数据的质量就越好。
因此，在极大似然估计中，我们的任务就是通过最小化负对数似然损失函数，找到最能够解释观测数据分布的参数θ，从而得到最优的参数估计值。
	
参考：
> https://zhuanlan.zhihu.com/p/74874291